{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd095b58-8282-4f2e-bac6-283ea6ba7b26",
   "metadata": {},
   "source": [
    "<img src=\"./img/hpe_logo.png\" alt=\"HPE Logo\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d7f34d-fd2b-4ca9-acab-6e04eddce9af",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Demo of the Enterprise Machine Learning/Data Science Platform</h1>\n",
    "\n",
    "<h5>Date: 08/24/23</h5>\n",
    "<h5>Version: 1.0</h5>\n",
    "<h5>Author(s): andrew.mendez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e943c221-3a68-461e-9e16-5208b5bc0dd3",
   "metadata": {},
   "source": [
    "# Overview\n",
    "## We will build an End to End ML Pipeline to train and deploy AI to detect Aircraft in Satellite Imagery\n",
    "<img src=\"./img/model-pred.png\" alt=\"Enterprise Machine Learning platform architecture\" width=\"850\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b8d13-30c7-467f-a42f-e620244ad888",
   "metadata": {},
   "source": [
    "# How will we build this?\n",
    "<img src=\"./img/PDK_Demo_Overview.png\" alt=\"Enterprise Machine Learning platform architecture\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f638760d-5175-4af4-b2b3-191dee209ecd",
   "metadata": {},
   "source": [
    "# What are they Key Challenges with delivering world class AI at the speed of the mission\n",
    "\n",
    "## Going from a research project to production\n",
    "## How can you equip teams of researchers\n",
    "\n",
    "* Good Data, Good Models, and Good Infrastructure\n",
    "* **Good Infrastructure is the hardest, and the biggest issue that cause 80-90% of ML projects to fail**\n",
    "\n",
    "\n",
    "What does Good AI Infrastructure Look Like:\n",
    "* Data Infrastructure:\n",
    "    * Data Management\n",
    "    * Data Versioning\n",
    "    * Pipeline Orchestration\n",
    "* Training Infrastructure\n",
    "    * Resource Management\n",
    "    * Distributed Training\n",
    "    * Fault Tolerance and Resumption\n",
    "    * Experiment Tracking\n",
    "    * IDE for developers to develop models\n",
    "    * Logging and Visualizing Metrics\n",
    "    * Hyper-parameter search\n",
    "* Serving Infrastructure\n",
    "    * Automatic Deployments\n",
    "    * Versioning Models\n",
    "    * Automatic Resource Allocation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092b67f-99e1-4dde-95dc-18a06ee80e1f",
   "metadata": {},
   "source": [
    "# How the HPE Machine Learning/Data Science Platform Helps:\n",
    "\n",
    "Built off two leading AI software Pachyderm and Determined.AI via acquisition.\n",
    "\n",
    "## What does the HPE ML/DS Plaform provide:\n",
    "* Data Infrastructure (Pachyderm):\n",
    "    * Data Management\n",
    "    * Data Versioning\n",
    "    * Pipeline Orchestration\n",
    "* Training Infrastructure (Determined.AI)\n",
    "    * Resource Management\n",
    "    * Distributed Training\n",
    "    * Multi-node training (LLMs)\n",
    "    * Fault Tolerance and Resumption at scale\n",
    "    * Experiment Tracking\n",
    "    * IDE for developers to develop models\n",
    "    * Logging and Visualizing Metrics\n",
    "    * Hyper-parameter search at scale\n",
    "* Serving Infrastructure (KServe)\n",
    "    * Automatic Deployments\n",
    "    * Versioning Models\n",
    "    * Automatic Resource Allocation\n",
    "    \n",
    "## End Result:\n",
    "* Less Lines of Code to manage data, train, and deploy at scale\n",
    "* Out of the box support for distributed training, hyperparameter search, and experiment tracking\n",
    "* Ensure the productivity and success of your team delivering great models at the speed of the mission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952bd553-5b75-42d0-bda7-5dc3e8d0be2a",
   "metadata": {},
   "source": [
    "<img src=\"./img/platform_step3.png\" alt=\"Enterprise Machine Learning platform architecture\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f1ec5-395c-46c1-a0a8-3f78f3800ab4",
   "metadata": {},
   "source": [
    "# Outline:\n",
    "* Create Data Repo and Upload data\n",
    "* Data Exploration\n",
    "* Create Simple Baseline\n",
    "* Train Model on MLDE\n",
    "* Migrate Training and Inference into Scalable, Automated E2E Ops: Model deployment for inference\n",
    "* Trigger End to End Pipeline Execution with updated better data\n",
    "* Trigger End to End Pipeline Execution with updated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb7fc75-f6eb-46cb-90fe-b73e94200bcf",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>Import modules and define functions</h3>\n",
    "The cell below imports all modules and libraries required to run the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b973bb3-0593-45a4-bfec-2b937b0472b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General modules\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "# Torch modules\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "import json\n",
    "# Image modules\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Import functions for downloading data\n",
    "from load_data import download_pach_repo, download_data, get_train_transforms\n",
    "\n",
    "# Import Determined Client\n",
    "from determined.experimental import client as det\n",
    "from determined import pytorch\n",
    "from utils import calculate_coco_stats, visualize_coco_annotations, load_model\n",
    "# Remove warnings\n",
    "import warnings\n",
    "\n",
    "import python_pachyderm\n",
    "from python_pachyderm.service import pps_proto\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619aa07c-4e2e-4261-b057-f6358197b740",
   "metadata": {},
   "source": [
    "<h1>Interactive Experimentation </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec21c452-fce2-4f21-b4eb-94db548e37ba",
   "metadata": {},
   "source": [
    "<img src=\"./img/platform_step1.png\" alt=\"Enterprise Machine Learning platform architecture\" width=\"850\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff16d2-b737-46db-ac59-29e5df4f2e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for Pachyderm connection\n",
    "pachyderm_host = \"10.182.1.45\"\n",
    "pachyderm_port = 80\n",
    "repo = \"data\"\n",
    "data_dir = './data/xview_3class_full/'\n",
    "branch = \"master\"\n",
    "project = \"object-detection-demo\"\n",
    "download_dir = \"./data_downloaded\"\n",
    "token = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8a470-1076-4eb3-abcc-170fbebf7854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new project\n",
    "client = python_pachyderm.Client(\n",
    "    host=pachyderm_host, port=pachyderm_port, auth_token=token\n",
    ")\n",
    "# client.create_project(project)# Un-comment if you have not already created a project named: object-detection-demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691afb4-f4cb-4674-8573-db5d731bfa93",
   "metadata": {},
   "source": [
    "# Create Data Repo and Upload data to Pachyderm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d624b6-c19b-4780-8d52-a9aef5370c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    client.create_repo(repo,project_name=project)# This line creates a Pachyderm Repo\n",
    "except Exception as e:\n",
    "    print(\"Failed to create data repo: {}...will try to upload to existing repo\".format(repo))\n",
    "    pass\n",
    "\n",
    "source_dir  = data_dir\n",
    "# Populate the input repos\n",
    "def insert_data(client,name,source_data_dir,project):\n",
    "    print(\"Inserting {} data...\".format(name))\n",
    "    with client.commit(name, \"master\",project_name=project) as c:\n",
    "        # data_dir = \"{}_data\".format(name)\n",
    "        python_pachyderm.put_files(client, source_data_dir, c, \"/\")\n",
    "                \n",
    "        return c\n",
    "\n",
    "d_commit = insert_data(client,repo,source_data_dir=source_dir,project=project)\n",
    "\n",
    "# Wait for the commits to finish\n",
    "print(\"Waiting for commits to finish...\")\n",
    "for commit in [client.wait_commit(c.id)[0] for c in [d_commit]]:\n",
    "    print(commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b239ceb4-76d8-4c7f-afdd-3844bbc32482",
   "metadata": {},
   "source": [
    "<h3>Inspect first version of data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeacbb51-99d1-493e-8359-a748f5dc9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call download, store paths in files\n",
    "files = download_data(pachyderm_host, pachyderm_port, repo, branch, project, download_dir, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f3f884-43ca-4acd-8c5d-d2d6220655b6",
   "metadata": {},
   "source": [
    "<h3> Some light data exploration</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb9dee-01ba-433d-bcd0-48c1aec10ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH='./data_downloaded/train_images_rgb_no_neg_filt_32/train_640_02_filtered_32.json'\n",
    "(num_annotations, \n",
    " cat_ids, \n",
    " cat_names, \n",
    " annotations_per_category, \n",
    " min_annotations, \n",
    " max_annotations, \n",
    " average_annotations, \n",
    " min_annotation_area,\n",
    " max_annotation_area,\n",
    " avg_annotation_area,\n",
    " min_annotation_area_per_category, \n",
    " max_annotation_area_per_category) = calculate_coco_stats(FILE_PATH)\n",
    "print(\"Number of images:\", num_annotations)\n",
    "print(f\"Number of Unique Categories: {len(cat_ids)}\")\n",
    "print(cat_ids)  # The IDs are not necessarily consecutive.\n",
    "\n",
    "print(\"Category IDs:\")\n",
    "print(\"Categories Names: \", cat_names)\n",
    "# Print or use annotations_per_category as needed\n",
    "print(\"Number of annotations per category:\")\n",
    "print(annotations_per_category)\n",
    "# Print or use the calculated metrics as needed\n",
    "print(\"Minimum number of annotations for an image:\", min_annotations)\n",
    "print(\"Maximum number of annotations in an image:\", max_annotations)\n",
    "print(\"Average number of annotations per image:\", average_annotations)\n",
    "\n",
    "print(\"Minimum annotation area:\", min_annotation_area)\n",
    "print(\"Maximum annotation area:\", max_annotation_area)\n",
    "print(\"Average annotation area:\", avg_annotation_area)\n",
    "\n",
    "print(\"\\nMinimum annotation area per category:\")\n",
    "print(min_annotation_area_per_category)\n",
    "\n",
    "print(\"\\nMaximum annotation area per category:\")\n",
    "print(max_annotation_area_per_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278d809-2f42-4142-8dbc-a7889ee4fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_ann_file_path = './data_downloaded/train_images_rgb_no_neg_filt_32/train_640_02_filtered_32.json'\n",
    "coco_img_dir = './data_downloaded/train_images_rgb_no_neg_filt_32/train_images_640_02_filt_32'\n",
    "visualize_coco_annotations(coco_ann_file_path, coco_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb49ceb5-310e-4c11-a572-01c8107c4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.models.detection\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "\n",
    "# from detection_utils.coco import get_coco, get_coco_kp\n",
    "from detection_utils.data import build_xview_dataset_filtered\n",
    "from detection_utils.group_by_aspect_ratio import GroupedBatchSampler, create_aspect_ratio_groups\n",
    "from detection_utils.engine import train_and_eval, eval_model\n",
    "\n",
    "# from detection_utils.train import get_dataset, get_transform\n",
    "from detection_utils.models import build_frcnn_model\n",
    "from PIL import Image\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from attrdict import AttrDict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a658e1-eee2-4673-a727-719c77204a7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2> Create Simple Baseline</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca21873-88d0-4a30-b60f-fd207735a193",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create data set class (inherit from Pytorch dataset)\n",
    "```python\n",
    "class LocalBackend:\n",
    "    \"\"\"\n",
    "    This class will load data from harddrive.\n",
    "    COCO dataset will be downloaded from source in model_def.py if\n",
    "    local backend is specified.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, outdir):\n",
    "        assert os.path.isdir(outdir)\n",
    "        self.outdir = outdir\n",
    "\n",
    "    def get(self, filepath):\n",
    "        with open(os.path.join(self.outdir, filepath), \"rb\") as f:\n",
    "            img_str = f.read()\n",
    "        return img_str\n",
    "\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backend,\n",
    "        root_dir,\n",
    "        img_folder,\n",
    "        ann_file,\n",
    "        transforms,\n",
    "        return_masks,\n",
    "        catIds=[],\n",
    "    ):\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "        self.img_folder = img_folder\n",
    "        self._transforms = transforms\n",
    "        self.prepare = ConvertCocoPolysToMask(return_masks)\n",
    "        elif backend == \"local\":\n",
    "            self.backend = LocalBackend(root_dir)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.catIds = catIds\n",
    "        self.catIds = self.coco.getCatIds()\n",
    "        '''\n",
    "        Remapping to set background class to zero, so can support FasterRCNN models\n",
    "        '''\n",
    "        self.catIdtoCls = {\n",
    "            catId: i+1 for i, catId in zip(range(len(self.catIds)), self.catIds)\n",
    "        }\n",
    "        self.clstoCatId = {\n",
    "            v:k for k,v in self.catIdtoCls.items()\n",
    "        }\n",
    "        self.num_classes = len(list(self.catIdtoCls.values()))+1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[idx]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id, catIds=self.catIds)\n",
    "        target = coco.loadAnns(ann_ids)\n",
    "        path = coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "        img_bytes = BytesIO(self.backend.get(os.path.join(self.img_folder, path)))\n",
    "\n",
    "        img = Image.open(img_bytes).convert(\"RGB\")\n",
    "        # img.save('test.png')\n",
    "        image_id = self.ids[idx]\n",
    "        target = {\"image_id\": image_id, \"annotations\": target}\n",
    "        img, target = self.prepare(img, target)\n",
    "        if self._transforms is not None:\n",
    "            img, target = self._transforms(img, target)\n",
    "        target[\"labels\"] = torch.tensor(\n",
    "                [self.catIdtoCls[l.item()] for l in target[\"labels\"]], dtype=torch.int64\n",
    "            )\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8944f-8b81-4068-b7c8-ceedd865f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_collate_fn(batch):\n",
    "    batch = list(zip(*batch))\n",
    "    return tuple(batch)\n",
    "\n",
    "data_dir = os.path.join('.', \"data_downloaded\")\n",
    "print(data_dir)\n",
    "dataset, num_classes = build_xview_dataset_filtered(image_set='train',args=AttrDict({\n",
    "                                                'data_dir':data_dir,\n",
    "                                                'backend':'local',\n",
    "                                                'masks': None,\n",
    "                                                }))\n",
    "print(\"--num_classes: \",num_classes)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "                                 dataset, \n",
    "                                 batch_size=16,\n",
    "                                 batch_sampler=None,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=1, \n",
    "                                 collate_fn=unwrap_collate_fn)\n",
    "print(data_dir)\n",
    "dataset_test, _ = build_xview_dataset_filtered(image_set='val',args=AttrDict({\n",
    "                                                'data_dir':data_dir,\n",
    "                                                'backend':'local',\n",
    "                                                'masks': None,\n",
    "                                                }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd291adf-2d61-4b56-8d72-baca7ee89f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2008c65-9540-4611-95ea-1e7d30c38d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5708df5a-ea6d-4933-a70c-ef86a7f53e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels used to translate category_id\n",
    "# det_labels = [\"Fixed-wing Aircraft\", \"Cargo Plane\"]\n",
    "det_labels = cat_names\n",
    "print(\"det_labels: \",det_labels)\n",
    "# Get one sample from the data set and show the image for validation\n",
    "for sample in data_loader:\n",
    "    img, targets = sample\n",
    "    img = img[0].cpu()\n",
    "    boxes = targets[0]['boxes'].cpu().numpy()\n",
    "    labels = targets[0]['labels'].cpu().numpy()\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.set_title(\"Example\")\n",
    "    ax.imshow(np.clip(img.permute(1, 2, 0), 0, 1))\n",
    "    # Plot bounding boxes as rectangles on the image\n",
    "    for box, label in zip(boxes, labels):\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                             fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_min, y_min - 5, f'Label: {det_labels[label-1]}', color='red', fontsize=10)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa86e64-76af-487f-a938-d57651f7cd58",
   "metadata": {},
   "source": [
    "<h3>Step 4: Create DataLoader and model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14049f32-76c8-414b-900e-739127e73191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pytorch data loader\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "                                 dataset, \n",
    "                                 batch_size=16,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=1, \n",
    "                                 collate_fn=unwrap_collate_fn)\n",
    "print(\"NUMBER OF BATCHES IN COCO: \",len(data_loader))# 59143, 7392 for mini coco\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "                            dataset_test,\n",
    "                            batch_size=4,\n",
    "                            shuffle=False,\n",
    "                            num_workers=1,\n",
    "                            collate_fn=unwrap_collate_fn)\n",
    "\n",
    "# Load FasterRCNN model (untrained) to GPU\n",
    "model = build_frcnn_model(num_classes=num_classes).cuda()\n",
    "device = 'cuda'\n",
    "\n",
    "# Setup loss function and optimizer. Note: the loss function is built into the torchvision FasterRCNN model\n",
    "optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=0.02,\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-4\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51807b4-108a-4a6a-a3e9-f2a98ed4ef47",
   "metadata": {},
   "source": [
    "<h3>Run some training (overfitting on one batch)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1fb5a-e5a2-4040-86eb-3cf9ee244083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iterator for data loader\n",
    "dataiter = iter(data_loader)\n",
    "\n",
    "# Get a single batch to overfit on\n",
    "inputs, targets = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06429b9-ce0c-42ae-a463-78a84599d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on dataset 2 times\n",
    "import time\n",
    "loss_dict = {'loss_classifier':1, 'loss_box_reg': 1, 'loss_objectness': 1, 'loss_rpn_box_reg': 1, 'tr_time': 1}\n",
    "# Initialize running sums and counts for each loss component\n",
    "running_sums = {key: 0.0 for key in loss_dict.keys()}\n",
    "running_counts = {key: 0 for key in loss_dict.keys()}\n",
    "NUM_EPOCHS=40\n",
    "for e in range(NUM_EPOCHS):\n",
    "    for ind, batch in enumerate(data_loader):\n",
    "        batch_time_start = time.time()\n",
    "        images, targets = batch\n",
    "        images = list(image.to(device, non_blocking=True) for image in images)\n",
    "        targets = [{k: v.to(device, non_blocking=True) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses_reduced = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses_reduced.item()\n",
    "        optimizer.zero_grad()\n",
    "        losses_reduced.backward()\n",
    "        optimizer.step()\n",
    "        total_batch_time = time.time() - batch_time_start\n",
    "\n",
    "        # Update running sums and counts for each loss component\n",
    "        for key, value in loss_dict.items():\n",
    "            running_sums[key] += value.item()\n",
    "            running_counts[key] += 1\n",
    "\n",
    "        # Calculate the running mean for each loss component\n",
    "        running_means = {key: running_sums[key] / running_counts[key] for key in loss_dict.keys()}\n",
    "\n",
    "        # Add the total batch time to the loss_dict\n",
    "        running_means['tr_time'] = total_batch_time\n",
    "        if ind%4 == 0:\n",
    "            # Print the contents in a nice, formatted manner\n",
    "            print(f\"Epoch {e}/{NUM_EPOCHS}\")\n",
    "            print(f\"{ind}/{len(data_loader)} \", end=\"\")\n",
    "            for key, value in running_means.items():\n",
    "                if key not in ['accuracy', 'val_loss', 'val_accuracy']:\n",
    "                    print(f\"{key}: {value:.4f} - \", end=\"\")\n",
    "            print()\n",
    "        # break\n",
    "\n",
    "print('\\nFinished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a06482-986b-4882-b140-40f58d5196ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(),'model_{}_low_lr_3class.pth'.format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91190b24-4abc-4899-81cb-f71521e28b25",
   "metadata": {},
   "source": [
    "<h3> Validate model trained in notebook</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de79bb96-db26-4a7b-9a59-d9558b15708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "# det_labels = [\"Fixed-wing Aircraft\", \"Cargo Plane\"]\n",
    "det_labels = cat_names\n",
    "print(\"det_labels: \",det_labels)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample in data_loader_test:\n",
    "        images, targets = sample\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        print(len(images))\n",
    "        fig, axs = plt.subplots(len(images), 2, figsize=(12, 6 * len(images)))\n",
    "\n",
    "        for idx, (img, target) in enumerate(zip(images, targets)):\n",
    "            img_np = img.clone().cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "            # Ground truth\n",
    "            axs[idx, 0].set_title(\"Ground Truth\")\n",
    "            axs[idx, 0].imshow(np.clip(img_np, 0, 1))\n",
    "            boxes_gt = target['boxes'].cpu().numpy()\n",
    "            labels_gt = target['labels'].cpu().numpy()\n",
    "            # print(boxes_gt)\n",
    "            for box, label in zip(boxes_gt, labels_gt):\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                                     fill=False, edgecolor='red', linewidth=2, alpha=0.5)\n",
    "                axs[idx, 0].add_patch(rect)\n",
    "                axs[idx, 0].text(x_min, y_min - 5, f'Label: {det_labels[label-1]}', color='red', fontsize=10)\n",
    "\n",
    "            # Model prediction\n",
    "            axs[idx, 1].set_title(\"Model Prediction\")\n",
    "            axs[idx, 1].imshow(np.clip(img_np, 0, 1))\n",
    "            outputs = model([img])\n",
    "            print(outputs[0])\n",
    "            if outputs[0]['boxes'].nelement() !=0:\n",
    "                outputs = [{k: v[0].to('cpu') for k, v in output.items()} for output in outputs]\n",
    "                boxes_pred = [outputs[0]['boxes'].numpy()] if not isinstance(outputs[0]['boxes'].numpy(),list) else outputs[0]['boxes'].numpy()\n",
    "                labels_pred = [outputs[0]['labels'].numpy()] if not isinstance(outputs[0]['labels'].numpy(),list) else outputs[0]['labels'].numpy()\n",
    "                scores_pred = [outputs[0]['scores'].numpy()] if not isinstance(outputs[0]['scores'].numpy(),list) else outputs[0]['scores'].numpy()\n",
    "\n",
    "                # Plot bounding boxes with scores greater than 0.05 as rectangles on the image\n",
    "                # print(boxes_pred, labels_pred, scores_pred)\n",
    "                for box, label, score in zip(boxes_pred, labels_pred, scores_pred):\n",
    "                    if score > 0.005:\n",
    "                        x_min, y_min, x_max, y_max = box\n",
    "                        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                                             fill=False, edgecolor='green', linewidth=2, alpha=0.5)\n",
    "                        axs[idx, 1].add_patch(rect)\n",
    "                        axs[idx, 1].text(x_min, y_min - 5, f'Label: {det_labels[label-1]}, Score: {score:.2f}', color='green', fontsize=12)\n",
    "\n",
    "                plt.axis('off')\n",
    "                # plt.show()\n",
    "            else:\n",
    "                pass\n",
    "                # plt.axis('off')\n",
    "        plt.show()\n",
    "            # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2926ace4-e640-421b-ac91-d410fd04fac2",
   "metadata": {},
   "source": [
    "# How ML Engineers integrate models into MLDE\n",
    "<img src=\"./img/determined_workflow.png\" alt=\"Enterprise Machine Learning platform architecture\" width=\"850\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3821153-bfa9-404d-880d-0ae0c65e4a2d",
   "metadata": {},
   "source": [
    "```python\n",
    "from determined.pytorch import (\n",
    "    DataLoader,\n",
    "    LRScheduler,\n",
    "    PyTorchTrial,\n",
    "    PyTorchTrialContext,\n",
    "    MetricReducer,\n",
    ")\n",
    "...\n",
    "class ObjectDetectionTrial(PyTorchTrial):\n",
    "    def __init__(self, context: PyTorchTrialContext) -> None:\n",
    "        self.context = context\n",
    "        self.hparams = AttrDict(self.context.get_hparams())\n",
    "        print(self.hparams) \n",
    "        ...\n",
    "        self.model = self.context.wrap_model(model)\n",
    "        \n",
    "        # wrap optimizer\n",
    "        self.optimizer = self.context.wrap_optimizer(optimizer)\n",
    "\n",
    "        scheduler_cls = WarmupWrapper(MultiStepLR)\n",
    "        ...\n",
    "        self.scheduler = self.context.wrap_lr_scheduler(\n",
    "            scheduler, step_mode=LRScheduler.StepMode.MANUAL_STEP\n",
    "        )\n",
    "        ...\n",
    "    def build_training_data_loader(self) -> DataLoader:\n",
    "        ...\n",
    "        return data_loader\n",
    "\n",
    "    def build_validation_data_loader(self) -> DataLoader:\n",
    "        ...\n",
    "        \n",
    "        return data_loader_test\n",
    "    \n",
    "    def train_batch(self, batch: TorchData, epoch_idx: int, batch_idx: int) -> Dict[str, torch.Tensor]:\n",
    "        batch_time_start = time.time()\n",
    "        images, targets = batch\n",
    "        ...\n",
    "        loss_dict = self.model(images, targets)\n",
    "        losses_reduced = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses_reduced.item()\n",
    "        self.context.backward(losses_reduced)\n",
    "        self.context.step_optimizer(self.optimizer)\n",
    "        self.scheduler.step()\n",
    "        loss_dict['lr'] = self.scheduler.get_lr()[0]\n",
    "\n",
    "        return loss_dict\n",
    "    \n",
    "    def evaluate_batch(self, batch: TorchData,batch_idx: int) -> Dict[str, Any]:\n",
    "        images, targets = batch\n",
    "        model_time_start = time.time()\n",
    "        # loss_dict, outputs = self.model(images, targets)\n",
    "        loss_dict = {}\n",
    "        loss_dict['eval_loss']=0.0\n",
    "        outputs = self.model(images, targets)\n",
    "        ...\n",
    "        self.reducer.update(result)\n",
    "        ...\n",
    "        return loss_dict\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6af95c-3642-4d22-822c-df52551674a1",
   "metadata": {},
   "source": [
    "<h1>Migrate Training into MLDE</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c71edd-0942-40f4-a733-a4bc611ff990",
   "metadata": {},
   "source": [
    "<img src=\"./img/platform_step2.png\" alt=\"Enterprise Machine Learning platform architecture\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600bb7f6-5270-43ae-a121-0bf52295be7e",
   "metadata": {},
   "source": [
    "<h3>Connect to our cluster and submit experiment on our AI at Scale Training platform</h3>\n",
    "This will take a few minutes because we are completing the following tasks:\n",
    "\n",
    "* Uploading experiment artifacts\n",
    "* Cluster allocates GPU resources and downloads docker container that was specified in experiment config \n",
    "* some additional python packages need to be installed\n",
    "* then training kicks off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4afb091-13ff-43fa-9cf6-1658171cf616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment using yaml file and submit to MLDE\n",
    "exp = det.create_experiment(config=\"./experiment/const.yaml\", model_dir=\"./experiment/\")\n",
    "print(f\"started experiment {exp.id}\")\n",
    "\n",
    "# Wait for experiment to complete and print exit status\n",
    "exit_status = exp.wait()\n",
    "print(f\"experiment completed with status {exit_status}\")\n",
    "\n",
    "# Get the best Checkpoint of the experiment and print uuid\n",
    "best_checkpoint = exp.top_checkpoint()\n",
    "best_checkpoint_uuid = best_checkpoint.uuid\n",
    "print(f\"Best checkpoint was {best_checkpoint_uuid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c05457-e44f-4708-8f98-73de6bde419e",
   "metadata": {},
   "source": [
    "<h3>Validate model trained on MLDE</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016490d-a3aa-40a3-95ea-e24660edf15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull model checkpoint from Determined.AI using UUID\n",
    "best_checkpoint = det.get_checkpoint(best_checkpoint_uuid)\n",
    "print(\"Best checkpoint retrieved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6373a77-d31e-4d64-a51c-b9d85bd0e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint to model\n",
    "path = best_checkpoint.download()\n",
    "print(path)\n",
    "N_CLASSES = num_classes + 1# Add one to include background class\n",
    "trained_model = build_frcnn_model(N_CLASSES)\n",
    "trained_model = load_model(trained_model,path)\n",
    "print(\"Checkpoint loaded into model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf937af-b50a-4736-9fdf-f974882fe7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "trained_model.to('cuda')\n",
    "trained_model.eval()\n",
    "# det_labels = [\"Fixed-wing Aircraft\", \"Cargo Plane\"]\n",
    "\n",
    "det_labels = cat_names\n",
    "print(\"det_labels: \",det_labels)\n",
    "\n",
    "device = 'cuda'\n",
    "with torch.no_grad():\n",
    "    for sample in data_loader_test:\n",
    "        images, targets = sample\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        fig, axs = plt.subplots(len(images), 2, figsize=(12, 6 * len(images)))\n",
    "\n",
    "        for idx, (img, target) in enumerate(zip(images, targets)):\n",
    "            img_np = img.clone().cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "            # Ground truth\n",
    "            axs[idx, 0].set_title(\"Ground Truth\")\n",
    "            axs[idx, 0].imshow(np.clip(img_np, 0, 1))\n",
    "            boxes_gt = target['boxes'].cpu().numpy()\n",
    "            labels_gt = target['labels'].cpu().numpy()\n",
    "            # print(boxes_gt)\n",
    "            for box, label in zip(boxes_gt, labels_gt):\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                                     fill=False, edgecolor='red', linewidth=2, alpha=0.5)\n",
    "                axs[idx, 0].add_patch(rect)\n",
    "                axs[idx, 0].text(x_min, y_min - 5, f'Label: {det_labels[label-1]}', color='red', fontsize=10)\n",
    "\n",
    "            # Model prediction\n",
    "            axs[idx, 1].set_title(\"Model Prediction\")\n",
    "            axs[idx, 1].imshow(np.clip(img_np, 0, 1))\n",
    "            outputs = trained_model([img])\n",
    "            outputs = [{k: v[0].to('cpu') for k, v in output.items()} for output in outputs]\n",
    "            boxes_pred = [outputs[0]['boxes'].numpy()] if not isinstance(outputs[0]['boxes'].numpy(),list) else outputs[0]['boxes'].numpy()\n",
    "            labels_pred = [outputs[0]['labels'].numpy()] if not isinstance(outputs[0]['labels'].numpy(),list) else outputs[0]['labels'].numpy()\n",
    "            scores_pred = [outputs[0]['scores'].numpy()] if not isinstance(outputs[0]['scores'].numpy(),list) else outputs[0]['scores'].numpy()\n",
    "\n",
    "            # Plot bounding boxes with scores greater than 0.05 as rectangles on the image\n",
    "            # print(boxes_pred, labels_pred, scores_pred)\n",
    "            for box, label, score in zip(boxes_pred, labels_pred, scores_pred):\n",
    "                if score > 0.05:\n",
    "                    x_min, y_min, x_max, y_max = box\n",
    "                    rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                                         fill=False, edgecolor='green', linewidth=2, alpha=0.5)\n",
    "                    axs[idx, 1].add_patch(rect)\n",
    "                    axs[idx, 1].text(x_min, y_min - 5, f'Label: {det_labels[label-1]}, Score: {score:.2f}', color='green', fontsize=12)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346bb18-dd48-4113-a515-b8dd16dfc7ce",
   "metadata": {},
   "source": [
    "<h1>Migrate Training and Inference into Scalable, Automated E2E Ops: Model deployment for inference</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00f8c0-6427-4176-9477-30709b64e392",
   "metadata": {},
   "source": [
    "<img src=\"./img/platform_step3.png\" alt=\"Enterprise Machine Learning platform architecture\" width=\"850\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82521382-74ab-430c-be89-2131b4f2c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = python_pachyderm.Client(\n",
    "    host=pachyderm_host, port=pachyderm_port, auth_token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cd3b5c-0451-4597-88b9-5890d45d60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline:\n",
    "import json\n",
    "PATH = 'pdk-use-cases-dev/object-detection/pipelines/training-pipeline.json'\n",
    "with open(PATH,'r') as file:\n",
    "    spec_d = json.loads(file.read())\n",
    "    spec = python_pachyderm.parse_dict_pipeline_spec(spec_d)\n",
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c7031-6cfb-46f8-a9a4-3976a730ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_pipeline(\n",
    "    pipeline_name=spec.pipeline.name,\n",
    "    transform=spec.transform,\n",
    "    project_name=project,\n",
    "    input=spec.input,\n",
    "    description=spec.description,\n",
    "    pod_patch = spec.pod_patch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b0c54-f713-4add-abbd-5d55f11e89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "PATH = 'pdk-use-cases-dev/object-detection/pipelines/deployment-pipeline.json'\n",
    "with open(PATH,'r') as file:\n",
    "    spec_d = json.loads(file.read())\n",
    "    spec = python_pachyderm.parse_dict_pipeline_spec(spec_d)\n",
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab2f0c-2776-4a18-8cd8-1a8d08c8055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_pipeline(\n",
    "    pipeline_name=spec.pipeline.name,\n",
    "    transform=spec.transform,\n",
    "    project_name=project,\n",
    "    input=spec.input,\n",
    "    description=spec.description,\n",
    "    pod_patch = spec.pod_patch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618163a5-8dc8-4097-8d12-9293cdecf06e",
   "metadata": {},
   "source": [
    "Show how on the MLDM pipeline UI, both training and deployment pipeline are created. We will need to wait a few minutes for each pipeline step to complete, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb35fe-6676-4db9-b16c-282ee83962f8",
   "metadata": {},
   "source": [
    "## Lets Look at model predicitions with our deployed model in another notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f574764-02b6-4d9f-94a4-2b9dc2d8ad3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Trigger End to End Pipeline Execution with updated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d21a1-4e11-4dea-9c06-30728f0ef583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import python_pachyderm\n",
    "from python_pachyderm.service import pps_proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a59821-7790-480c-b37b-3f4d3c5fcb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for Pachyderm connection\n",
    "pachyderm_host = \"10.182.1.45\"\n",
    "pachyderm_port = 80\n",
    "repo = \"data\"\n",
    "branch = \"master\"\n",
    "project = \"object-detection-demo\"\n",
    "download_dir = \"./xview_dataset_full\"\n",
    "data_dir = './data/xview_2class_full/'\n",
    "token = \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f2cda-bc65-4258-8bf1-ef3c7cf66639",
   "metadata": {},
   "source": [
    "## Here we have a new version of the dataset that includes more labels (88 annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1512a03-25ee-4909-a64d-092d7d41708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH=f'{data_dir}/train_images_rgb_no_neg_filt_32/train_640_02_filtered_32.json'\n",
    "(num_annotations, \n",
    " cat_ids, \n",
    " cat_names, \n",
    " annotations_per_category, \n",
    " min_annotations, \n",
    " max_annotations, \n",
    " average_annotations, \n",
    " min_annotation_area,\n",
    " max_annotation_area,\n",
    " avg_annotation_area,\n",
    " min_annotation_area_per_category, \n",
    " max_annotation_area_per_category) = calculate_coco_stats(FILE_PATH)\n",
    "print(\"Number of images:\", num_annotations)\n",
    "print(f\"Number of Unique Categories: {len(cat_ids)}\")\n",
    "print(cat_ids)  # The IDs are not necessarily consecutive.\n",
    "\n",
    "print(\"Category IDs:\")\n",
    "print(\"Categories Names: \", cat_names)\n",
    "# Print or use annotations_per_category as needed\n",
    "print(\"Number of annotations per category:\")\n",
    "print(annotations_per_category)\n",
    "# Print or use the calculated metrics as needed\n",
    "print(\"Minimum number of annotations for an image:\", min_annotations)\n",
    "print(\"Maximum number of annotations in an image:\", max_annotations)\n",
    "print(\"Average number of annotations per image:\", average_annotations)\n",
    "\n",
    "print(\"Minimum annotation area:\", min_annotation_area)\n",
    "print(\"Maximum annotation area:\", max_annotation_area)\n",
    "print(\"Average annotation area:\", avg_annotation_area)\n",
    "\n",
    "print(\"\\nMinimum annotation area per category:\")\n",
    "print(min_annotation_area_per_category)\n",
    "\n",
    "print(\"\\nMaximum annotation area per category:\")\n",
    "print(max_annotation_area_per_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a1225-be9f-41cd-a035-940bc14a3f8c",
   "metadata": {},
   "source": [
    "We will show how when we upload an updated version of our dataset, the entire end-to-end training and deployment pipeline kicks off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a3793-e9ff-49b8-adca-bd87b0e60ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = python_pachyderm.Client(\n",
    "    host=pachyderm_host, port=pachyderm_port, auth_token=token\n",
    ")\n",
    "source_dir  = data_dir\n",
    "def insert_data(client,name,source_data_dir,project):\n",
    "    print(\"Inserting {} data...\".format(name))\n",
    "    with client.commit(name, \"master\",project_name=project) as c:\n",
    "        # data_dir = \"{}_data\".format(name)\n",
    "        python_pachyderm.put_files(client, source_data_dir, c, \"/\")\n",
    "                \n",
    "        return c\n",
    "# name = \"objdet-data2\"\n",
    "d_commit = insert_data(client,repo,source_data_dir=download_dir\n",
    "                       \n",
    "                       ,project=project)\n",
    "\n",
    "# Wait for the commits to finish\n",
    "print(\"Waiting for commits to finish...\")\n",
    "for commit in [client.wait_commit(c.id)[0] for c in [d_commit]]:\n",
    "    print(commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1167c9b8-909a-46f7-9af7-cb543cf20e47",
   "metadata": {},
   "source": [
    "## Run cell to clean up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5933c362-e4a0-46c8-b39d-49672c1e91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete pachyderm pipelines\n",
    "import json\n",
    "\n",
    "PATH = 'pdk-use-cases-dev/object-detection/pipelines/deployment-pipeline.json'\n",
    "with open(PATH,'r') as file:\n",
    "    spec_d = json.loads(file.read())\n",
    "    spec = python_pachyderm.parse_dict_pipeline_spec(spec_d)\n",
    "# delete deployment pipeline\n",
    "client.delete_pipeline(\n",
    "        pipeline_name=spec.pipeline.name,\n",
    "        force = False,\n",
    "        keep_repo = False,\n",
    "        project_name = 'object-detection-demo',\n",
    "    )\n",
    "print(\"Deleted pipeline: {}\".format(spec.pipeline.name))\n",
    "\n",
    "\n",
    "PATH = 'pdk-use-cases-dev/object-detection/pipelines/training-pipeline.json'\n",
    "with open(PATH,'r') as file:\n",
    "    spec_d = json.loads(file.read())\n",
    "    spec = python_pachyderm.parse_dict_pipeline_spec(spec_d)\n",
    "\n",
    "# delete training pipeline\n",
    "client.delete_pipeline(\n",
    "        pipeline_name=spec.pipeline.name,\n",
    "        force = False,\n",
    "        keep_repo = False,\n",
    "        project_name = 'object-detection-demo',\n",
    "    )\n",
    "print(\"Deleted pipeline: {}\".format(spec.pipeline.name))\n",
    "\n",
    "# delete pachyderm repo\n",
    "client.delete_repo(\n",
    "        repo_name=repo, \n",
    "        force = False, \n",
    "        project_name = 'object-detection-demo',\n",
    "    )\n",
    "print(\"Deleted repo: {}\".format(repo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f083108-6370-4b41-adfd-8866645a5824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2640636-ea6d-4f48-b2b8-c2ad7d51c964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
